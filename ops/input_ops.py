# -*- coding: utf-8 -*-

#Author: Jay Yip
#Date 04Mar2017

"""Batching, padding and masking the input sequence and output sequence"""

from __future__ import absolute_import
from __future__ import division
from __future__ import print_function


import tensorflow as tf

def parse_example_queue(example_queue, context_feature_name, tag_feature_name):
  """ Read one example.
  This function read one example and return context sequence and tag sequence
  correspondingly. 

  Args:
    filename_queue: A filename queue returned by string_input_producer
    context_feature_name: Context feature name in TFRecord. Set in ModelConfig
    tag_feature_name: Tag feature name in TFRecord. Set in ModelConfig

  Returns:
    input_seq: An int32 Tensor with different length.
    tag_seq: An int32 Tensor with the shape of [4].
  """
  #Read TFRecord from filename queue
  #_, serialized_example = reader.read(filename_queue)
  serialized_example = example_queue.dequeue()

  #Parse one example
  _, features = tf.parse_sequence_example(serialized_example, 
    sequence_features = {
      context_feature_name: tf.FixedLenSequenceFeature([], dtype=tf.int64),
      tag_feature_name: tf.FixedLenSequenceFeature([4], dtype = tf.int64)
    })

  return (features[context_feature_name], features[tag_feature_name])


def batch_with_dynamic_pad(images_and_captions,
                           batch_size,
                           queue_capacity,
                           add_summaries=True):
  """Batches input images and captions.

  This function splits the caption into an input sequence and a target sequence,
  where the target sequence is the input sequence right-shifted by 1. Input and
  target sequences are batched and padded up to the maximum length of sequences
  in the batch. A mask is created to distinguish real words from padding words.

  Example:
    Actual captions in the batch ('-' denotes padded character):
      [
        [ 1 2 5 4 5 ],
        [ 1 2 3 4 - ],
        [ 1 2 3 - - ],
      ]

    input_seqs:
      [
        [ 1 2 3 4 ],
        [ 1 2 3 - ],
        [ 1 2 - - ],
      ]

    target_seqs:
      [
        [ 2 3 4 5 ],
        [ 2 3 4 - ],
        [ 2 3 - - ],
      ]

    mask:
      [
        [ 1 1 1 1 ],
        [ 1 1 1 0 ],
        [ 1 1 0 0 ],
      ]

  Args:
    images_and_captions: A list of pairs [image, caption], where image is a
      Tensor of shape [height, width, channels] and caption is a 1-D Tensor of
      any length. Each pair will be processed and added to the queue in a
      separate thread.
    batch_size: Batch size.
    queue_capacity: Queue capacity.
    add_summaries: If true, add caption length summaries.

  Returns:
    images: A Tensor of shape [batch_size, height, width, channels].
    input_seqs: An int32 Tensor of shape [batch_size, padded_length].
    target_seqs: An int32 Tensor of shape [batch_size, padded_length].
    mask: An int32 0/1 Tensor of shape [batch_size, padded_length].
  """
  enqueue_list = []
  for image, caption in images_and_captions:
    caption_length = tf.shape(caption)[0]
    input_length = tf.expand_dims(tf.subtract(caption_length, 1), 0)

    input_seq = tf.slice(caption, [0], input_length)
    target_seq = tf.slice(caption, [1], input_length)
    indicator = tf.ones(input_length, dtype=tf.int32)
    enqueue_list.append([image, input_seq, target_seq, indicator])

  images, input_seqs, target_seqs, mask = tf.train.batch_join(
      enqueue_list,
      batch_size=batch_size,
      capacity=queue_capacity,
      dynamic_pad=True,
      name="batch_and_pad")

  if add_summaries:
    lengths = tf.add(tf.reduce_sum(mask, 1), 1)
    tf.summary.scalar("caption_length/batch_min", tf.reduce_min(lengths))
    tf.summary.scalar("caption_length/batch_max", tf.reduce_max(lengths))
    tf.summary.scalar("caption_length/batch_mean", tf.reduce_mean(lengths))

  return input_seqs, target_seqs, mask


def example_queue_shuffle(reader, filename_queue, is_training, example_queue_name = 'example_queue', capacity = 50000, num_reader_threads = 1):
  """
  This function shuffle the examples within the filename queues. Since there's no 
  padding option in shuffle_batch, we have to manually shuffle the example queue.

  The process is given as below.
  create filename queue >> read examples from filename queue >> enqueue example to example queue(RandomShuffleQueue)

  However, this is not totally random shuffle since the memory limiation. Therefore, 
  we need to specify a capacity of the example queue.

  Args:
    reader: A TFRecord Reader
    filename_queue: A queue generated by string_input_producer
    is_traning: If not training then use FIFOqueue(No need to shuffle).
    example_queue_name: Name of the example queue
    capacity: Value queue capacity. Should be large enough for better mixing
    num_reader_threads: Number of thread to enqueue the value queue

  Returns:
    example_queue: An example queue that is shuffled. Ready for parsing and batching.
  """

  #Init queue
  if is_training:
    example_queue = tf.RandomShuffleQueue(
        capacity=capacity,
        min_after_dequeue=min_queue_examples,
        dtypes=[tf.string],
        name="random_" + example_queue_name)
  else:
    example_queue = tf.FIFOQueue(
        capacity=capacity, dtypes=[tf.string], name="fifo_" + example_queue_name)

  #Manually create ops to enqueue
  enqueue_example_ops = []
  for _ in range(num_reader_threads):
    _, example = reader.read(filename_queue)
    enqueue_example_ops.append(example_queue.enqueue([example]))

  #Add queue runner
  tf.train.queue_runner.add_queue_runner(tf.train.queue_runner.QueueRunner(
      example_queue, enqueue_ops))
  tf.summary.scalar(
      "queue/%s/fraction_of_%d_full" % (example_queue.name, capacity),
      tf.cast(example_queue.size(), tf.float32) * (1. / capacity))

  return example_queue



